{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-10T18:00:36.317778Z",
     "start_time": "2025-05-10T18:00:35.718222Z"
    }
   },
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from models_monopoly import CentralizedDQNAgent\n",
    "from Monopoly_Go.monopoly_go import monopoly_go_v0  \n",
    "\n",
    "env = monopoly_go_v0.env(render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# --- Config ---\n",
    "n_agents = 3\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "update_freq = 5  # update every N episodes\n",
    "obs_dim = env.observe(\"player_0\").shape[0]\n",
    "act_dim = 611\n",
    "\n",
    "# --- Agent ---\n",
    "cdqn = CentralizedDQNAgent(obs_dim=obs_dim, act_dim=act_dim, n_agents=n_agents, lr=1e-5)\n",
    "\n",
    "# --- Tracking ---\n",
    "winners = [0] * n_agents\n",
    "win_rates = []\n",
    "losses = []"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T18:00:36.591118Z",
     "start_time": "2025-05-10T18:00:36.584715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent_vs_random(cdqn, env_fn, eval_episodes=100, agent_idx=0):\n",
    "    \"\"\"\n",
    "    Evaluate a trained CDQN agent against random opponents.\n",
    "\n",
    "    Args:\n",
    "        cdqn: CentralizedDQNAgent instance.\n",
    "        env_fn: Function that returns a new PettingZoo env (e.g. monopoly_go_v0.env).\n",
    "        eval_episodes: Number of evaluation games to run.\n",
    "        agent_idx: The index of the agent you want to evaluate (e.g., player_0 â†’ 0).\n",
    "\n",
    "    Returns:\n",
    "        Win rate of the evaluated agent.\n",
    "    \"\"\"\n",
    "    n_agents = cdqn.n_agents\n",
    "    wins = 0\n",
    "\n",
    "    for ep in range(eval_episodes):\n",
    "        env = env_fn(render_mode=None)\n",
    "        env.reset(seed=10000 + ep)\n",
    "        # env.shaped = True\n",
    "        terminated = [False] * n_agents\n",
    "\n",
    "        while not all(terminated):\n",
    "            curr_agent = env.agent_selection\n",
    "            curr_idx = env.curr_agent_index\n",
    "            obs, _, term, _, info = env.last()\n",
    "\n",
    "            if term:\n",
    "                terminated[curr_idx] = True\n",
    "                env.step(None)\n",
    "                continue\n",
    "\n",
    "            full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "            masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "\n",
    "            if curr_idx == agent_idx:\n",
    "                action = cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "            else:\n",
    "                legal_actions = np.where(masks[curr_idx])[0]\n",
    "                action = int(np.random.choice(legal_actions)) if len(legal_actions) > 0 else None\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        if getattr(env, \"winner\", -1) == agent_idx:\n",
    "            wins += 1\n",
    "\n",
    "    win_rate = wins / eval_episodes\n",
    "    print(f\"[Evaluation] Agent {agent_idx} win rate vs randoms: {win_rate:.2f}\")\n",
    "    return win_rate\n"
   ],
   "id": "51fe12ae6ba8a343",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T18:00:37.444728Z",
     "start_time": "2025-05-10T18:00:37.439101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent_vs_past_self(cdqn, old_cdqn, env_fn, eval_episodes=100, agent_idx=0):\n",
    "    \"\"\"\n",
    "    Evaluate a trained CDQN agent against older versions of itself.\n",
    "    \n",
    "    Args:\n",
    "        cdqn: The current agent to evaluate.\n",
    "        old_cdqn: A frozen (non-updating) copy of an earlier CDQN.\n",
    "        env_fn: Function that returns a new PettingZoo env.\n",
    "        eval_episodes: Number of games to evaluate.\n",
    "        agent_idx: Which agent you're evaluating.\n",
    "    \"\"\"\n",
    "    n_agents = cdqn.n_agents\n",
    "    wins = 0\n",
    "\n",
    "    for ep in range(eval_episodes):\n",
    "        env = env_fn(render_mode=None)\n",
    "        env.reset(seed=10000 + ep)\n",
    "        terminated = [False] * n_agents\n",
    "\n",
    "        while not all(terminated):\n",
    "            curr_agent = env.agent_selection\n",
    "            curr_idx = env.curr_agent_index\n",
    "            obs, _, term, _, info = env.last()\n",
    "\n",
    "            if term:\n",
    "                terminated[curr_idx] = True\n",
    "                env.step(None)\n",
    "                continue\n",
    "\n",
    "            full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "            masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "\n",
    "            if curr_idx == agent_idx:\n",
    "                action = cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "            else:\n",
    "                action = old_cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        if getattr(env, \"winner\", -1) == agent_idx:\n",
    "            wins += 1\n",
    "\n",
    "    win_rate = wins / eval_episodes\n",
    "    print(f\"[Eval vs past self] Agent {agent_idx} win rate: {win_rate:.2f}\")\n",
    "    return win_rate\n"
   ],
   "id": "df63b81796970a03",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-10T18:59:06.589499Z",
     "start_time": "2025-05-10T18:14:55.241983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_rewards = []\n",
    "old_cdqn = None\n",
    "average_rewards = [[], [], []]\n",
    "for ep in range(num_episodes):\n",
    "    env = monopoly_go_v0.env(render_mode=None)\n",
    "    env.reset(seed=ep)\n",
    "    env.shaped = False\n",
    "\n",
    "    episode_rewards = [0] * n_agents\n",
    "    terminated = [False] * n_agents\n",
    "\n",
    "    # Initialize full observation and mask per agent\n",
    "    full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "    masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "    assert all(m.sum() > 0 for m in masks)\n",
    "    \n",
    "    while True:\n",
    "        curr_agent = env.agent_selection\n",
    "        curr_idx = env.curr_agent_index\n",
    "        obs, reward, term, trunc, info = env.last()\n",
    "\n",
    "        if term:\n",
    "            terminated[curr_idx] = True\n",
    "            env.step(None)\n",
    "\n",
    "        # Get centralized state (all obs + masks)\n",
    "        full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "        masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "        assert masks[curr_idx].sum() > 0\n",
    "\n",
    "        # Select only the current agent's action using CDQN\n",
    "        if not term:\n",
    "            action = cdqn.select(full_obs, epsilon, masks, curr_idx)\n",
    "            assert masks[curr_idx][action] == 1, f\"Agent {curr_idx} chose masked action {action}\"\n",
    "\n",
    "            # Step with the selected action\n",
    "            env.step(action)\n",
    "\n",
    "        # Log transition for centralized training\n",
    "        next_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "        rewards = [env._cumulative_rewards[f\"player_{i}\"] for i in range(n_agents)]\n",
    "        dones = [env.terminations[f\"player_{i}\"] for i in range(n_agents)]\n",
    "\n",
    "        # Store entire transition (centralized)\n",
    "        cdqn.store(full_obs, [action if i == curr_idx else -1 for i in range(n_agents)],\n",
    "                   rewards, next_obs, dones)\n",
    "\n",
    "        # Update running obs + rewards\n",
    "        full_obs = next_obs\n",
    "        episode_rewards = [r + ep_r for r, ep_r in zip(rewards, episode_rewards)]\n",
    "        \n",
    "        if all(terminated):\n",
    "            all_rewards.append(sum(episode_rewards))\n",
    "            break\n",
    "\n",
    "    # Learn\n",
    "    if ep % update_freq == 0:\n",
    "        cdqn.update()\n",
    "\n",
    "    # Epsilon decay\n",
    "    if ep > 500:\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Logging\n",
    "    winner_idx = getattr(env, \"winner\", -1)\n",
    "    if winner_idx >= 0:\n",
    "        winners[winner_idx] += 1\n",
    "\n",
    "    if ep % 100 == 0 and ep > 0:\n",
    "        win_rate = [w / 100 for w in winners]\n",
    "        win_rates.append(win_rate)\n",
    "        print(f\"[EP {ep}] Win rates: {win_rate}, Epsilon: {epsilon:.3f}\")\n",
    "        reward_avg = np.mean(all_rewards[-100:])\n",
    "        print(f\"[EP {ep}] Avg reward: {reward_avg:.2f}\")\n",
    "\n",
    "        winners = [0] * n_agents\n",
    "    \n",
    "    if ep % 250 == 0 and ep > 0:\n",
    "        old_cdqn = copy.deepcopy(cdqn)\n",
    "        \n",
    "    if ep % 500 == 0 and ep > 0:\n",
    "        for i in range(3):\n",
    "            env = monopoly_go_v0.env\n",
    "            evaluate_agent_vs_random(cdqn, env, agent_idx=i)"
   ],
   "id": "b478352b61059377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 100] Win rates: [0.62, 0.67, 0.52], Epsilon: 0.050\n",
      "[EP 100] Avg reward: -10.00\n",
      "[EP 200] Win rates: [0.43, 0.3, 0.27], Epsilon: 0.050\n",
      "[EP 200] Avg reward: -10.00\n",
      "[EP 300] Win rates: [0.28, 0.4, 0.32], Epsilon: 0.050\n",
      "[EP 300] Avg reward: -10.00\n",
      "[EP 400] Win rates: [0.34, 0.35, 0.31], Epsilon: 0.050\n",
      "[EP 400] Avg reward: -10.00\n",
      "[EP 500] Win rates: [0.33, 0.36, 0.31], Epsilon: 0.050\n",
      "[EP 500] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.01\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.01\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.01\n",
      "[EP 600] Win rates: [0.23, 0.4, 0.37], Epsilon: 0.050\n",
      "[EP 600] Avg reward: -10.00\n",
      "[EP 700] Win rates: [0.33, 0.42, 0.25], Epsilon: 0.050\n",
      "[EP 700] Avg reward: -10.00\n",
      "[EP 800] Win rates: [0.37, 0.32, 0.31], Epsilon: 0.050\n",
      "[EP 800] Avg reward: -10.00\n",
      "[EP 900] Win rates: [0.23, 0.47, 0.3], Epsilon: 0.050\n",
      "[EP 900] Avg reward: -10.00\n",
      "[EP 1000] Win rates: [0.31, 0.46, 0.23], Epsilon: 0.050\n",
      "[EP 1000] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.03\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.01\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 1100] Win rates: [0.22, 0.44, 0.34], Epsilon: 0.050\n",
      "[EP 1100] Avg reward: -10.00\n",
      "[EP 1200] Win rates: [0.34, 0.49, 0.17], Epsilon: 0.050\n",
      "[EP 1200] Avg reward: -10.00\n",
      "[EP 1300] Win rates: [0.26, 0.53, 0.21], Epsilon: 0.050\n",
      "[EP 1300] Avg reward: -10.00\n",
      "[EP 1400] Win rates: [0.23, 0.51, 0.26], Epsilon: 0.050\n",
      "[EP 1400] Avg reward: -10.00\n",
      "[EP 1500] Win rates: [0.33, 0.46, 0.21], Epsilon: 0.050\n",
      "[EP 1500] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.01\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 1600] Win rates: [0.25, 0.56, 0.18], Epsilon: 0.050\n",
      "[EP 1600] Avg reward: -9.98\n",
      "[EP 1700] Win rates: [0.27, 0.56, 0.17], Epsilon: 0.050\n",
      "[EP 1700] Avg reward: -10.00\n",
      "[EP 1800] Win rates: [0.27, 0.57, 0.16], Epsilon: 0.050\n",
      "[EP 1800] Avg reward: -10.00\n",
      "[EP 1900] Win rates: [0.21, 0.65, 0.14], Epsilon: 0.050\n",
      "[EP 1900] Avg reward: -10.00\n",
      "[EP 2000] Win rates: [0.23, 0.6, 0.16], Epsilon: 0.050\n",
      "[EP 2000] Avg reward: -9.98\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 2100] Win rates: [0.22, 0.6, 0.18], Epsilon: 0.050\n",
      "[EP 2100] Avg reward: -10.00\n",
      "[EP 2200] Win rates: [0.24, 0.61, 0.15], Epsilon: 0.050\n",
      "[EP 2200] Avg reward: -10.00\n",
      "[EP 2300] Win rates: [0.13, 0.77, 0.1], Epsilon: 0.050\n",
      "[EP 2300] Avg reward: -10.00\n",
      "[EP 2400] Win rates: [0.22, 0.57, 0.21], Epsilon: 0.050\n",
      "[EP 2400] Avg reward: -10.00\n",
      "[EP 2500] Win rates: [0.18, 0.68, 0.14], Epsilon: 0.050\n",
      "[EP 2500] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 2600] Win rates: [0.27, 0.61, 0.11], Epsilon: 0.050\n",
      "[EP 2600] Avg reward: -9.98\n",
      "[EP 2700] Win rates: [0.28, 0.59, 0.13], Epsilon: 0.050\n",
      "[EP 2700] Avg reward: -10.00\n",
      "[EP 2800] Win rates: [0.22, 0.7, 0.08], Epsilon: 0.050\n",
      "[EP 2800] Avg reward: -10.00\n",
      "[EP 2900] Win rates: [0.28, 0.65, 0.06], Epsilon: 0.050\n",
      "[EP 2900] Avg reward: -9.98\n",
      "[EP 3000] Win rates: [0.21, 0.7, 0.09], Epsilon: 0.050\n",
      "[EP 3000] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 3100] Win rates: [0.17, 0.73, 0.1], Epsilon: 0.050\n",
      "[EP 3100] Avg reward: -10.00\n",
      "[EP 3200] Win rates: [0.24, 0.65, 0.1], Epsilon: 0.050\n",
      "[EP 3200] Avg reward: -9.98\n",
      "[EP 3300] Win rates: [0.15, 0.75, 0.1], Epsilon: 0.050\n",
      "[EP 3300] Avg reward: -10.00\n",
      "[EP 3400] Win rates: [0.21, 0.75, 0.04], Epsilon: 0.050\n",
      "[EP 3400] Avg reward: -10.00\n",
      "[EP 3500] Win rates: [0.24, 0.63, 0.13], Epsilon: 0.050\n",
      "[EP 3500] Avg reward: -10.00\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 3600] Win rates: [0.3, 0.65, 0.05], Epsilon: 0.050\n",
      "[EP 3600] Avg reward: -10.00\n",
      "[EP 3700] Win rates: [0.27, 0.64, 0.09], Epsilon: 0.050\n",
      "[EP 3700] Avg reward: -10.00\n",
      "[EP 3800] Win rates: [0.24, 0.67, 0.08], Epsilon: 0.050\n",
      "[EP 3800] Avg reward: -9.98\n",
      "[EP 3900] Win rates: [0.28, 0.68, 0.04], Epsilon: 0.050\n",
      "[EP 3900] Avg reward: -10.00\n",
      "[EP 4000] Win rates: [0.22, 0.69, 0.07], Epsilon: 0.050\n",
      "[EP 4000] Avg reward: -9.96\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.00\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.00\n",
      "[EP 4100] Win rates: [0.21, 0.71, 0.07], Epsilon: 0.050\n",
      "[EP 4100] Avg reward: -9.98\n",
      "[EP 4200] Win rates: [0.28, 0.62, 0.1], Epsilon: 0.050\n",
      "[EP 4200] Avg reward: -10.00\n",
      "[EP 4300] Win rates: [0.25, 0.68, 0.05], Epsilon: 0.050\n",
      "[EP 4300] Avg reward: -9.96\n",
      "[EP 4400] Win rates: [0.34, 0.56, 0.09], Epsilon: 0.050\n",
      "[EP 4400] Avg reward: -9.98\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 27\u001B[39m\n\u001B[32m     24\u001B[39m     env.step(\u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[32m     26\u001B[39m \u001B[38;5;66;03m# Get centralized state (all obs + masks)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m27\u001B[39m full_obs = [\u001B[43menv\u001B[49m\u001B[43m.\u001B[49m\u001B[43mobserve\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mplayer_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_agents)]\n\u001B[32m     28\u001B[39m masks = [env.infos[\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mplayer_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m][\u001B[33m\"\u001B[39m\u001B[33maction_mask\u001B[39m\u001B[33m\"\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_agents)]\n\u001B[32m     29\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m masks[curr_idx].sum() > \u001B[32m0\u001B[39m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Programming/RL_Models/Monopoly_Go/monopoly_go/env/monopoly_go.py:377\u001B[39m, in \u001B[36mMonopolyGoEnv.observe\u001B[39m\u001B[34m(self, agent)\u001B[39m\n\u001B[32m    373\u001B[39m         other_banks[k] = \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mself\u001B[39m.state[agent][\u001B[33m\"\u001B[39m\u001B[33mother_banks\u001B[39m\u001B[33m\"\u001B[39m][k])\n\u001B[32m    375\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m np.concatenate((hand, bank, properties, other_properties, other_banks))\n\u001B[32m--> \u001B[39m\u001B[32m377\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfill_action_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    378\u001B[39m fill_other_info()\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m simpler_flat_observation()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Programming/RL_Models/Monopoly_Go/monopoly_go/env/monopoly_go.py:289\u001B[39m, in \u001B[36mfill_action_mask\u001B[39m\u001B[34m(self, agent)\u001B[39m\n\u001B[32m    287\u001B[39m hand = \u001B[38;5;28mself\u001B[39m.state[agent][\u001B[33m\"\u001B[39m\u001B[33mhand\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    288\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.must_discard:\n\u001B[32m--> \u001B[39m\u001B[32m289\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m card_id \u001B[38;5;129;01min\u001B[39;00m hand:\n\u001B[32m    290\u001B[39m         action_mask[offsets[\u001B[33m\"\u001B[39m\u001B[33mdiscard_card\u001B[39m\u001B[33m\"\u001B[39m] + card_id] = \u001B[32m1\u001B[39m\n\u001B[32m    291\u001B[39m     \u001B[38;5;28mself\u001B[39m.infos[agent][\u001B[33m\"\u001B[39m\u001B[33maction_mask\u001B[39m\u001B[33m\"\u001B[39m] = action_mask\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/Desktop/Programming/RL_Models/Monopoly_Go/monopoly_go/env/monopoly_go.py:168\u001B[39m, in \u001B[36mmark_rent\u001B[39m\u001B[34m(card_id, has_double_the_rent)\u001B[39m\n\u001B[32m    165\u001B[39m             index = start + rel\n\u001B[32m    166\u001B[39m             logger.info(\n\u001B[32m    167\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mForced deal: marking index \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for your slot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00myour_slot_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, opponent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopp_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, their slot \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mopp_slot_idx\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m168\u001B[39m             action_mask[index] = \u001B[32m1\u001B[39m\n\u001B[32m    169\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m np.sum(action_mask[offsets[\u001B[33m\"\u001B[39m\u001B[33mforced_deal\u001B[39m\u001B[33m\"\u001B[39m]:offsets[\u001B[33m\"\u001B[39m\u001B[33mforced_deal\u001B[39m\u001B[33m\"\u001B[39m] + \u001B[32m200\u001B[39m]) > \u001B[32m0\u001B[39m:\n\u001B[32m    170\u001B[39m     \u001B[38;5;28mself\u001B[39m.action_availabilities[agent][\u001B[33m\"\u001B[39m\u001B[33mforced_deal\u001B[39m\u001B[33m\"\u001B[39m] = \\\n\u001B[32m    171\u001B[39m         \u001B[38;5;28mself\u001B[39m.action_availabilities[agent].get(\u001B[33m\"\u001B[39m\u001B[33mforced_deal\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0\u001B[39m) + \u001B[32m1\u001B[39m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10a0ed7129ae0cc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
