{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-08T23:33:31.865966Z",
     "start_time": "2025-05-08T23:33:31.846484Z"
    }
   },
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "from models_monopoly import CentralizedDQNAgent\n",
    "from Monopoly_Go.monopoly_go import monopoly_go_v0  \n",
    "\n",
    "env = monopoly_go_v0.env(render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# --- Config ---\n",
    "n_agents = 3\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.995\n",
    "update_freq = 5  # update every N episodes\n",
    "obs_dim = env.observe(\"player_0\").shape[0]\n",
    "act_dim = 611\n",
    "\n",
    "# --- Agent ---\n",
    "cdqn = CentralizedDQNAgent(obs_dim=obs_dim, act_dim=act_dim, n_agents=n_agents, lr=1e-5)\n",
    "\n",
    "# --- Tracking ---\n",
    "winners = [0] * n_agents\n",
    "win_rates = []\n",
    "losses = []"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T23:33:31.946648Z",
     "start_time": "2025-05-08T23:33:31.941710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent_vs_random(cdqn, env_fn, eval_episodes=100, agent_idx=0):\n",
    "    \"\"\"\n",
    "    Evaluate a trained CDQN agent against random opponents.\n",
    "\n",
    "    Args:\n",
    "        cdqn: CentralizedDQNAgent instance.\n",
    "        env_fn: Function that returns a new PettingZoo env (e.g. monopoly_go_v0.env).\n",
    "        eval_episodes: Number of evaluation games to run.\n",
    "        agent_idx: The index of the agent you want to evaluate (e.g., player_0 â†’ 0).\n",
    "\n",
    "    Returns:\n",
    "        Win rate of the evaluated agent.\n",
    "    \"\"\"\n",
    "    n_agents = cdqn.n_agents\n",
    "    wins = 0\n",
    "\n",
    "    for ep in range(eval_episodes):\n",
    "        env = env_fn(render_mode=None)\n",
    "        env.reset(seed=10000 + ep)\n",
    "        # env.shaped = True\n",
    "        terminated = [False] * n_agents\n",
    "\n",
    "        while not all(terminated):\n",
    "            curr_agent = env.agent_selection\n",
    "            curr_idx = env.curr_agent_index\n",
    "            obs, _, term, _, info = env.last()\n",
    "\n",
    "            if term:\n",
    "                terminated[curr_idx] = True\n",
    "                env.step(None)\n",
    "                continue\n",
    "\n",
    "            full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "            masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "\n",
    "            if curr_idx == agent_idx:\n",
    "                action = cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "            else:\n",
    "                legal_actions = np.where(masks[curr_idx])[0]\n",
    "                action = int(np.random.choice(legal_actions)) if len(legal_actions) > 0 else None\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        if getattr(env, \"winner\", -1) == agent_idx:\n",
    "            wins += 1\n",
    "\n",
    "    win_rate = wins / eval_episodes\n",
    "    print(f\"[Evaluation] Agent {agent_idx} win rate vs randoms: {win_rate:.2f}\")\n",
    "    return win_rate\n"
   ],
   "id": "51fe12ae6ba8a343",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-08T23:33:32.069439Z",
     "start_time": "2025-05-08T23:33:32.065304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_agent_vs_past_self(cdqn, old_cdqn, env_fn, eval_episodes=100, agent_idx=0):\n",
    "    \"\"\"\n",
    "    Evaluate a trained CDQN agent against older versions of itself.\n",
    "    \n",
    "    Args:\n",
    "        cdqn: The current agent to evaluate.\n",
    "        old_cdqn: A frozen (non-updating) copy of an earlier CDQN.\n",
    "        env_fn: Function that returns a new PettingZoo env.\n",
    "        eval_episodes: Number of games to evaluate.\n",
    "        agent_idx: Which agent you're evaluating.\n",
    "    \"\"\"\n",
    "    n_agents = cdqn.n_agents\n",
    "    wins = 0\n",
    "\n",
    "    for ep in range(eval_episodes):\n",
    "        env = env_fn(render_mode=None)\n",
    "        env.reset(seed=10000 + ep)\n",
    "        terminated = [False] * n_agents\n",
    "\n",
    "        while not all(terminated):\n",
    "            curr_agent = env.agent_selection\n",
    "            curr_idx = env.curr_agent_index\n",
    "            obs, _, term, _, info = env.last()\n",
    "\n",
    "            if term:\n",
    "                terminated[curr_idx] = True\n",
    "                env.step(None)\n",
    "                continue\n",
    "\n",
    "            full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "            masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "\n",
    "            if curr_idx == agent_idx:\n",
    "                action = cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "            else:\n",
    "                action = old_cdqn.select(full_obs, eps=0.0, masks=masks, acting_agent=curr_idx)\n",
    "\n",
    "            env.step(action)\n",
    "\n",
    "        if getattr(env, \"winner\", -1) == agent_idx:\n",
    "            wins += 1\n",
    "\n",
    "    win_rate = wins / eval_episodes\n",
    "    print(f\"[Eval vs past self] Agent {agent_idx} win rate: {win_rate:.2f}\")\n",
    "    return win_rate\n"
   ],
   "id": "df63b81796970a03",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-05-08T23:33:32.185312Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_rewards = []\n",
    "old_cdqn = None\n",
    "for ep in range(num_episodes):\n",
    "    env = monopoly_go_v0.env(render_mode=None)\n",
    "    env.reset(seed=ep)\n",
    "    # if ep < 1000:\n",
    "    #     env.shaped = True\n",
    "\n",
    "    episode_rewards = [0] * n_agents\n",
    "    terminated = [False] * n_agents\n",
    "\n",
    "    # Initialize full observation and mask per agent\n",
    "    full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "    masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "    assert all(m.sum() > 0 for m in masks)\n",
    "    \n",
    "    while True:\n",
    "        curr_agent = env.agent_selection\n",
    "        curr_idx = env.curr_agent_index\n",
    "        obs, reward, term, trunc, info = env.last()\n",
    "\n",
    "        if term:\n",
    "            terminated[curr_idx] = True\n",
    "            env.step(None)\n",
    "\n",
    "        # Get centralized state (all obs + masks)\n",
    "        full_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "        masks = [env.infos[f\"player_{i}\"][\"action_mask\"] for i in range(n_agents)]\n",
    "        assert masks[curr_idx].sum() > 0\n",
    "\n",
    "        # Select only the current agent's action using CDQN\n",
    "        if not term:\n",
    "            action = cdqn.select(full_obs, epsilon, masks, curr_idx)\n",
    "            assert masks[curr_idx][action] == 1, f\"Agent {curr_idx} chose masked action {action}\"\n",
    "\n",
    "            # Step with the selected action\n",
    "            env.step(action)\n",
    "\n",
    "        # Log transition for centralized training\n",
    "        next_obs = [env.observe(f\"player_{i}\") for i in range(n_agents)]\n",
    "        rewards = [env._cumulative_rewards[f\"player_{i}\"] for i in range(n_agents)]\n",
    "        dones = [env.terminations[f\"player_{i}\"] for i in range(n_agents)]\n",
    "\n",
    "        # Store entire transition (centralized)\n",
    "        cdqn.store(full_obs, [action if i == curr_idx else -1 for i in range(n_agents)],\n",
    "                   rewards, next_obs, dones)\n",
    "\n",
    "        # Update running obs + rewards\n",
    "        full_obs = next_obs\n",
    "        episode_rewards = [r + ep_r for r, ep_r in zip(rewards, episode_rewards)]\n",
    "        \n",
    "        if all(terminated):\n",
    "            all_rewards.append(sum(episode_rewards))\n",
    "            break\n",
    "\n",
    "    # Learn\n",
    "    if ep % update_freq == 0:\n",
    "        cdqn.update()\n",
    "\n",
    "    # Epsilon decay\n",
    "    if ep > 500:\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "    # Logging\n",
    "    winner_idx = getattr(env, \"winner\", -1)\n",
    "    if winner_idx >= 0:\n",
    "        winners[winner_idx] += 1\n",
    "\n",
    "    if ep % 100 == 0 and ep > 0:\n",
    "        win_rate = [w / 100 for w in winners]\n",
    "        win_rates.append(win_rate)\n",
    "        print(f\"[EP {ep}] Win rates: {win_rate}, Epsilon: {epsilon:.3f}\")\n",
    "        reward_avg = np.mean(all_rewards[-100:])\n",
    "        print(f\"[EP {ep}] Avg reward: {reward_avg:.2f}\")\n",
    "\n",
    "        winners = [0] * n_agents\n",
    "    \n",
    "    if ep % 250 == 0 and ep > 0:\n",
    "        old_cdqn = copy.deepcopy(cdqn)\n",
    "        \n",
    "    if ep % 500 == 0 and ep > 0:\n",
    "        for i in range(3):\n",
    "            env = monopoly_go_v0.env\n",
    "            evaluate_agent_vs_past_self(cdqn, old_cdqn, env, agent_idx=i)\n",
    "            evaluate_agent_vs_random(cdqn, env, agent_idx=i)"
   ],
   "id": "b478352b61059377",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EP 100] Win rates: [0.38, 0.28, 0.31], Epsilon: 1.000\n",
      "[EP 100] Avg reward: -65.60\n",
      "[EP 200] Win rates: [0.34, 0.33, 0.3], Epsilon: 1.000\n",
      "[EP 200] Avg reward: -64.20\n",
      "[EP 300] Win rates: [0.27, 0.26, 0.45], Epsilon: 1.000\n",
      "[EP 300] Avg reward: -62.80\n",
      "[EP 400] Win rates: [0.36, 0.27, 0.34], Epsilon: 1.000\n",
      "[EP 400] Avg reward: -64.20\n",
      "[EP 500] Win rates: [0.27, 0.39, 0.31], Epsilon: 1.000\n",
      "[EP 500] Avg reward: -64.20\n",
      "[Eval vs past self] Agent 0 win rate: 0.25\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.05\n",
      "[Eval vs past self] Agent 1 win rate: 0.59\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.22\n",
      "[Eval vs past self] Agent 2 win rate: 0.21\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.05\n",
      "[EP 600] Win rates: [0.36, 0.33, 0.29], Epsilon: 0.606\n",
      "[EP 600] Avg reward: -62.80\n",
      "[EP 700] Win rates: [0.26, 0.52, 0.22], Epsilon: 0.367\n",
      "[EP 700] Avg reward: -60.00\n",
      "[EP 800] Win rates: [0.19, 0.62, 0.19], Epsilon: 0.222\n",
      "[EP 800] Avg reward: -60.00\n",
      "[EP 900] Win rates: [0.22, 0.57, 0.21], Epsilon: 0.135\n",
      "[EP 900] Avg reward: -60.00\n",
      "[EP 1000] Win rates: [0.27, 0.61, 0.12], Epsilon: 0.082\n",
      "[EP 1000] Avg reward: -60.00\n",
      "[Eval vs past self] Agent 0 win rate: 0.30\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.02\n",
      "[Eval vs past self] Agent 1 win rate: 0.65\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.19\n",
      "[Eval vs past self] Agent 2 win rate: 0.10\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.01\n",
      "[EP 1100] Win rates: [0.24, 0.65, 0.11], Epsilon: 0.050\n",
      "[EP 1100] Avg reward: -60.00\n",
      "[EP 1200] Win rates: [0.26, 0.61, 0.13], Epsilon: 0.050\n",
      "[EP 1200] Avg reward: -60.00\n",
      "[EP 1300] Win rates: [0.19, 0.65, 0.16], Epsilon: 0.050\n",
      "[EP 1300] Avg reward: -60.00\n",
      "[EP 1400] Win rates: [0.24, 0.63, 0.13], Epsilon: 0.050\n",
      "[EP 1400] Avg reward: -60.00\n",
      "[EP 1500] Win rates: [0.24, 0.65, 0.11], Epsilon: 0.050\n",
      "[EP 1500] Avg reward: -60.00\n",
      "[Eval vs past self] Agent 0 win rate: 0.24\n",
      "[Evaluation] Agent 0 win rate vs randoms: 0.01\n",
      "[Eval vs past self] Agent 1 win rate: 0.70\n",
      "[Evaluation] Agent 1 win rate vs randoms: 0.19\n",
      "[Eval vs past self] Agent 2 win rate: 0.06\n",
      "[Evaluation] Agent 2 win rate vs randoms: 0.03\n",
      "[EP 1600] Win rates: [0.26, 0.73, 0.01], Epsilon: 0.050\n",
      "[EP 1600] Avg reward: -60.00\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "10a0ed7129ae0cc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d41a51c241ce374"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
