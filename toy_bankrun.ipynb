{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "e522f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeef5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class BankRunEnv(gym.Env):\n",
    "    metadata = {'render.modes': []}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_agents: int = 5,\n",
    "        T: int = 10,\n",
    "        alpha: float = 0.3,\n",
    "        health_min: float = 0.0,\n",
    "        sigma: float = 0.1,\n",
    "        deposit_low: float = 0.5,\n",
    "        deposit_high: float = 1.5,\n",
    "        R_func=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_agents = n_agents\n",
    "        self.T = T\n",
    "        self.alpha = alpha\n",
    "        self.health_min = health_min\n",
    "        self.sigma = sigma\n",
    "        self.R_func = R_func or (lambda h: np.exp(h))\n",
    "\n",
    "        # obs = [noisy_health, last_withdraw_frac]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([-np.inf, 0.0], dtype=np.float32),\n",
    "            high=np.array([ np.inf, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        # two actions: wait or withdraw\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "\n",
    "        # each agentâ€™s fixed deposit, drawn once\n",
    "        self.fixed_deposits = np.random.uniform(\n",
    "            deposit_low, deposit_high, size=self.n_agents\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # random initial health in [0.5, 1.5]\n",
    "        self.initial_health = np.random.uniform(0.5, 1.5)\n",
    "        self.health = self.initial_health\n",
    "\n",
    "        self.t = 0\n",
    "        self.collapsed = False\n",
    "        self.last_withdraw_frac = 0.0\n",
    "\n",
    "        # reset deposits & total\n",
    "        self.deposits = self.fixed_deposits.copy()\n",
    "        self.total_deposits = float(self.deposits.sum())\n",
    "        self.active = np.ones(self.n_agents, dtype=bool)\n",
    "\n",
    "        return self._get_obs()\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # noisy health signal for each agent\n",
    "        noisy = self.health + np.random.randn(self.n_agents) * self.sigma\n",
    "        return [\n",
    "            np.array([noisy[i], self.last_withdraw_frac], dtype=np.float32)\n",
    "            for i in range(self.n_agents)\n",
    "        ]\n",
    "\n",
    "    def step(self, actions):\n",
    "        assert 0 <= self.t < self.T and not self.collapsed, \"Episode done\"\n",
    "        actions = np.array(actions, dtype=int)\n",
    "        rewards = np.zeros(self.n_agents, dtype=np.float32)\n",
    "\n",
    "        # intermediate rounds\n",
    "        if self.t < self.T - 1:\n",
    "            # who withdraws this step?\n",
    "            withdrawers = (actions == 1) & self.active\n",
    "            idx = np.where(withdrawers)[0].tolist()\n",
    "\n",
    "            # pay and deactivate\n",
    "            withdrawn_amt = 0.0\n",
    "            for i in idx:\n",
    "                rewards[i] = self.deposits[i]\n",
    "                withdrawn_amt += self.deposits[i]\n",
    "                self.active[i] = False\n",
    "\n",
    "            # multiplicative decay of health\n",
    "            if self.total_deposits > 0:\n",
    "                frac = withdrawn_amt / self.total_deposits\n",
    "                self.health *= max(0.0, 1.0 - self.alpha * frac)\n",
    "\n",
    "            self.last_withdraw_frac = len(idx) / float(self.n_agents)\n",
    "\n",
    "            # collapse check\n",
    "            if self.health <= self.health_min:\n",
    "                self.collapsed = True\n",
    "                self.active[:] = False\n",
    "\n",
    "            self.t += 1\n",
    "            done = not self.active.any()\n",
    "            obs = self._get_obs() if not done else [np.zeros(2, dtype=np.float32)] * self.n_agents\n",
    "            dones = [done] * self.n_agents\n",
    "            info = {'health': self.health, 'collapsed': self.collapsed}\n",
    "            return obs, rewards.tolist(), dones, info\n",
    "\n",
    "        # final period: everyone forced to exit\n",
    "        else:\n",
    "            remaining = self.active\n",
    "            Rpay = float(self.R_func(self.health))\n",
    "            rewards[remaining] = self.deposits[remaining] * Rpay\n",
    "            self.active[:] = False\n",
    "            self.t += 1\n",
    "            obs = [np.zeros(2, dtype=np.float32)] * self.n_agents\n",
    "            dones = [True] * self.n_agents\n",
    "            info = {'health': self.health, 'collapsed': self.collapsed}\n",
    "            return obs, rewards.tolist(), dones, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"t={self.t}/{self.T-1}, health={self.health:.3f}, collapsed={self.collapsed}\")\n",
    "        print(f\"Deposits: {self.deposits}\")\n",
    "        print(f\"Active: {self.active}, last_withdraw_frac={self.last_withdraw_frac:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "0d0d4926",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CentralizedQNet(nn.Module):\n",
    "    def __init__(self, n_agents, obs_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        input_dim = n_agents * obs_dim\n",
    "        output_dim = 2 ** n_agents\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "Transition = namedtuple('Transition', ('state','action','reward','next_state','done'))\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def index_to_joint_action(idx, n_agents):\n",
    "    b = format(idx, f'0{n_agents}b')\n",
    "    return tuple(int(x) for x in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "3d8a9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_power(h, rho=0.10, beta=2):\n",
    "    return 1.0 + rho * (h ** beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "id": "0d7f1626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_centralized_dqn(env, n_agents, obs_dim, buffer, q_net, target_q_net,\n",
    "                          optimizer, episodes=1000, batch_size=64,\n",
    "                          gamma=0.99, sync_every=50):\n",
    "    for ep in range(episodes):\n",
    "        joint_obs = np.concatenate(env.reset()).astype(np.float32)\n",
    "        done = False\n",
    "        while not done:\n",
    "            if random.random() < max(0.1, 1 - ep/episodes):\n",
    "                act_idx = random.randrange(2**n_agents)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    qs = q_net(torch.from_numpy(joint_obs).unsqueeze(0))\n",
    "                    act_idx = qs.argmax(dim=1).item()\n",
    "\n",
    "            joint_act = index_to_joint_action(act_idx, n_agents)\n",
    "            obs_, rewards, dones, info = env.step(joint_act)\n",
    "            joint_obs_next = np.concatenate(obs_).astype(np.float32)\n",
    "            r = sum(rewards)\n",
    "            buffer.push(joint_obs, act_idx, r, joint_obs_next, any(dones))\n",
    "            joint_obs = joint_obs_next\n",
    "            done = any(dones)\n",
    "\n",
    "        if len(buffer) >= batch_size:\n",
    "            batch = Transition(*zip(*buffer.sample(batch_size)))\n",
    "            s = torch.tensor(batch.state)\n",
    "            a = torch.tensor(batch.action).unsqueeze(1)\n",
    "            r = torch.tensor(batch.reward).unsqueeze(1)\n",
    "            s2 = torch.tensor(batch.next_state)\n",
    "            d = torch.tensor(batch.done).unsqueeze(1)\n",
    "\n",
    "            q_vals = q_net(s).gather(1, a)\n",
    "            with torch.no_grad():\n",
    "                next_q = target_q_net(s2).max(dim=1, keepdim=True)[0]\n",
    "                target = r + gamma * next_q * (~d)\n",
    "            loss = nn.MSELoss()(q_vals, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if ep % sync_every == 0:\n",
    "            target_q_net.load_state_dict(q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "1a0b92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, n_agents, q_net, episodes=5):\n",
    "    for ep in range(episodes):\n",
    "        joint_obs = np.concatenate(env.reset()).astype(np.float32)\n",
    "        done = False\n",
    "        step = 0\n",
    "        print(f\"\\n--- Eval Episode {ep} ---\")\n",
    "        print(env.health)\n",
    "        while not done:\n",
    "            with torch.no_grad():\n",
    "                qs = q_net(torch.from_numpy(joint_obs).unsqueeze(0))\n",
    "                act_idx = qs.argmax(dim=1).item()\n",
    "            joint_act = index_to_joint_action(act_idx, n_agents)\n",
    "            obs_, rewards, dones, info = env.step(joint_act)\n",
    "            print(f\"Step {step}: Health={info['health']:.3f}, Actions={joint_act}, Obs={[round(float(joint_obs[2*i]),3) for i in range(len(joint_obs)//2)]}, Rewards={[round(i,3) for i in rewards]}\")\n",
    "            joint_obs = np.concatenate(obs_).astype(np.float32)\n",
    "            done = any(dones)\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15880738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Eval Episode 0 ---\n",
      "1.1831454022188115\n",
      "Step 0: Health=1.068, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.849, 1.054, 0.736, 1.308, 1.148, 1.091, 1.337, 1.416], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=1.068, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.152, 0.628, 0.857, 0.893, 1.302, 1.161, 1.11, 1.219], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 2: Health=1.068, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.063, 0.931, 1.014, 0.993, 1.605, 1.053, 1.181, 0.782], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=0.757, Actions=(0, 0, 1, 0, 0, 0, 0, 1), Obs=[0.899, 0.695, 1.076, 0.913, 0.948, 0.647, 1.034, 1.148], Rewards=[0.0, 0.0, 0.731, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 4: Health=0.757, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.041, 0.738, 1.016, 0.554, 0.896, 0.918, 0.66, 0.507], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=0.588, Actions=(1, 1, 0, 0, 0, 1, 0, 0), Obs=[0.631, 0.896, 0.329, 0.425, 0.715, 0.591, 0.487, 0.605], Rewards=[0.682, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.514, Actions=(0, 1, 1, 0, 0, 0, 1, 0), Obs=[0.861, -0.029, 0.485, 1.014, 0.668, 0.805, 0.435, 0.512], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.965, 0.0]\n",
      "Step 7: Health=0.514, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.399, 0.628, 0.186, 0.843, 0.488, 0.256, 0.428, 0.39], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 8: Health=0.514, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.169, 0.871, 0.301, 0.567, 0.299, 1.002, 0.286, 0.004], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 9: Health=0.514, Actions=(1, 1, 1, 0, 0, 0, 0, 0), Obs=[0.232, 0.697, 0.615, 0.536, 0.509, 0.43, 0.735, 0.281], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 10: Health=0.514, Actions=(0, 0, 1, 0, 0, 0, 0, 1), Obs=[0.399, 0.565, 0.227, 0.895, 0.58, 0.996, 0.771, 1.018], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 11: Health=0.514, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.441, 0.509, 0.099, -0.041, 0.67, 0.125, 0.477, 0.521], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 12: Health=0.514, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.496, 0.715, 0.373, -0.077, 0.585, 0.319, 0.098, 1.253], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 13: Health=0.514, Actions=(0, 0, 1, 0, 0, 1, 1, 0), Obs=[0.843, 0.562, 0.275, 0.264, 0.698, 0.629, 0.822, 0.582], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 14: Health=0.514, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.215, 0.677, 0.554, 0.39, 0.482, 0.21, 0.456, 0.279], Rewards=[0.0, 0.0, 0.0, 1.7, 1.648, 0.0, 0.0, 0.0]\n",
      "\n",
      "--- Eval Episode 1 ---\n",
      "0.6267451634038252\n",
      "Step 0: Health=0.427, Actions=(0, 0, 1, 0, 0, 1, 1, 0), Obs=[0.63, 0.417, 0.474, 0.848, 0.756, 0.665, 0.862, 0.584], Rewards=[0.0, 0.0, 0.731, 0.0, 0.0, 0.748, 0.965, 0.0]\n",
      "Step 1: Health=0.248, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.873, 0.226, 0.402, 0.319, 0.736, 0.045, 0.649, 0.296], Rewards=[0.682, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "\n",
      "--- Eval Episode 2 ---\n",
      "1.1231319160841626\n",
      "Step 0: Health=0.796, Actions=(0, 0, 1, 0, 0, 0, 0, 1), Obs=[0.971, 0.594, 0.773, 1.073, 0.844, 1.366, 1.374, 0.984], Rewards=[0.0, 0.0, 0.731, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 1: Health=0.519, Actions=(1, 0, 0, 1, 0, 0, 1, 0), Obs=[0.546, 0.682, 0.899, 0.95, 0.811, 0.884, 0.87, 1.181], Rewards=[0.682, 0.0, 0.0, 1.017, 0.0, 0.0, 0.965, 0.0]\n",
      "Step 2: Health=0.449, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.597, 0.229, 0.291, 0.358, 0.675, 0.634, 0.433, 0.573], Rewards=[0.0, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=0.449, Actions=(1, 1, 1, 1, 0, 0, 0, 0), Obs=[0.791, 0.344, 0.389, 0.644, 0.346, 0.386, 0.669, 0.858], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=0.449, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.258, 0.609, 0.319, 0.863, -0.061, 0.414, 0.096, 0.529], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=0.449, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.443, 0.235, 0.305, 0.586, 0.635, 0.082, 0.327, 0.194], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.449, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.057, 0.227, 0.061, 0.414, 0.071, 0.59, 0.404, 0.196], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 7: Health=0.405, Actions=(1, 0, 0, 1, 0, 1, 0, 0), Obs=[0.365, 0.4, 0.483, 0.717, 0.495, 0.851, 0.412, 1.125], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 8: Health=0.405, Actions=(1, 1, 0, 0, 0, 1, 0, 0), Obs=[0.903, 0.817, 0.593, 0.389, 0.559, 0.578, 0.502, 0.43], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 9: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.659, 0.473, 0.118, 0.269, 0.524, 0.283, 0.733, 0.22], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 10: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.458, 0.484, -0.078, 0.524, 0.372, 0.293, 0.544, 0.443], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 11: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.416, 0.596, 0.241, 0.672, 0.481, 0.367, 0.36, 0.382], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 12: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.449, 0.834, 0.354, 0.438, 0.418, -0.027, 0.44, 0.398], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 13: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.059, 0.23, 0.472, 0.496, 0.325, 0.199, 0.257, 0.235], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 14: Health=0.405, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.375, 0.367, 0.39, 0.409, 1.09, 0.549, 0.413, 0.188], Rewards=[0.0, 0.0, 0.0, 0.0, 1.479, 0.0, 0.0, 0.0]\n",
      "\n",
      "--- Eval Episode 3 ---\n",
      "1.402979157925191\n",
      "Step 0: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.67, 1.439, 1.03, 1.444, 1.423, 1.154, 1.58, 1.529], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.305, 0.932, 1.245, 1.195, 1.22, 1.38, 1.467, 1.393], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 2: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.238, 1.579, 1.169, 1.127, 1.491, 1.272, 1.278, 1.405], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.401, 1.006, 1.185, 1.04, 1.479, 1.344, 1.275, 0.955], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.626, 1.326, 0.939, 1.55, 1.232, 1.049, 1.502, 0.787], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.129, 1.092, 1.161, 1.126, 1.167, 1.468, 1.282, 1.416], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.44, 1.371, 1.123, 1.236, 1.054, 1.442, 1.362, 0.995], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 7: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.239, 1.402, 0.998, 1.199, 1.64, 1.113, 1.195, 1.053], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 8: Health=1.266, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.57, 1.265, 1.007, 1.549, 0.922, 1.19, 1.294, 0.795], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 9: Health=0.775, Actions=(0, 0, 0, 1, 1, 0, 1, 0), Obs=[0.839, 1.557, 1.615, 1.294, 1.379, 1.186, 1.344, 1.275], Rewards=[0.0, 0.0, 0.0, 1.017, 0.986, 0.0, 0.965, 0.0]\n",
      "Step 10: Health=0.775, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[0.92, 0.547, 0.703, 0.792, 0.722, 0.331, 1.136, 0.579], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 11: Health=0.602, Actions=(1, 1, 0, 0, 0, 1, 0, 0), Obs=[1.045, 0.782, 0.81, 0.622, 0.817, 0.605, 0.587, 0.676], Rewards=[0.682, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 12: Health=0.484, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.657, 0.67, 0.515, 0.43, 0.406, 0.589, 0.5, 0.89], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 13: Health=0.438, Actions=(1, 1, 1, 0, 0, 0, 0, 0), Obs=[0.458, 0.987, 0.229, 0.78, 0.669, 0.51, 0.99, 0.428], Rewards=[0.0, 0.0, 0.731, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "--- Eval Episode 4 ---\n",
      "0.5609773245013315\n",
      "Step 0: Health=0.382, Actions=(1, 0, 0, 1, 0, 1, 0, 0), Obs=[0.528, 0.572, 0.624, 0.437, 0.66, 0.916, 0.585, 0.511], Rewards=[0.682, 0.0, 0.0, 1.017, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=0.256, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.672, 0.445, 0.156, 0.219, 0.469, 0.654, 0.367, 0.923], Rewards=[0.0, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "\n",
      "--- Eval Episode 5 ---\n",
      "1.0648562188596737\n",
      "Step 0: Health=0.961, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.179, 0.957, 0.805, 1.18, 1.33, 0.916, 1.49, 1.131], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=0.961, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[0.948, 0.835, 0.668, 0.572, 0.576, 0.502, 0.465, 0.957], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 2: Health=0.961, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.202, 1.035, 1.215, 1.076, 1.275, 0.783, 1.176, 0.752], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=0.961, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.057, 0.697, 0.644, 1.395, 0.938, 0.981, 1.152, 0.804], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=0.588, Actions=(0, 0, 0, 1, 1, 0, 1, 0), Obs=[0.983, 1.076, 0.803, 0.855, 0.82, 0.777, 0.521, 1.066], Rewards=[0.0, 0.0, 0.0, 1.017, 0.986, 0.0, 0.965, 0.0]\n",
      "Step 5: Health=0.453, Actions=(0, 1, 1, 0, 0, 0, 1, 0), Obs=[0.27, 0.83, 0.546, 0.869, 0.989, 0.365, 0.61, 0.745], Rewards=[0.0, 1.03, 0.731, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.453, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.116, 0.311, -0.233, 0.797, 0.778, 0.666, 0.166, 0.356], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 7: Health=0.453, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.377, 0.19, 0.714, 0.653, -0.089, 0.221, 0.29, 0.505], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 8: Health=0.413, Actions=(1, 1, 0, 0, 0, 1, 0, 0), Obs=[0.568, 0.427, 0.832, 0.415, 0.799, 0.498, 0.337, 0.294], Rewards=[0.682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 9: Health=0.413, Actions=(1, 1, 1, 1, 0, 0, 0, 0), Obs=[0.793, 0.433, 0.471, 0.593, 0.828, 0.03, 0.337, 0.588], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 10: Health=0.413, Actions=(1, 0, 0, 1, 0, 1, 0, 0), Obs=[0.419, 0.229, 0.679, 0.422, 0.81, 0.811, 0.745, 0.459], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 11: Health=0.413, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.152, 0.47, 0.709, 0.104, 0.839, 0.184, 0.362, 0.47], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 12: Health=0.413, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[-0.138, 0.405, 0.491, 0.465, 0.238, 0.319, 0.049, 0.689], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 13: Health=0.413, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.238, 0.323, 0.269, 0.506, 0.441, 0.559, 0.365, 0.654], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 14: Health=0.413, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[-0.152, 0.402, -0.25, 0.555, 0.512, 0.441, 0.064, 0.471], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.263]\n",
      "\n",
      "--- Eval Episode 6 ---\n",
      "1.132686905454136\n",
      "Step 0: Health=1.022, Actions=(0, 0, 0, 0, 0, 1, 0, 0), Obs=[1.267, 1.494, 1.028, 0.557, 1.08, 1.277, 1.015, 1.1], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=0.658, Actions=(0, 1, 1, 0, 0, 0, 1, 0), Obs=[1.334, 1.036, 0.89, 1.451, 1.501, 0.831, 0.569, 0.841], Rewards=[0.0, 1.03, 0.731, 0.0, 0.0, 0.0, 0.965, 0.0]\n",
      "Step 2: Health=0.471, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.665, 0.485, 0.577, 0.447, 0.765, 0.745, 0.787, 0.429], Rewards=[0.682, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 3: Health=0.471, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.457, 0.385, 0.089, 0.348, 0.043, 0.35, 0.331, 0.075], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=0.471, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.42, 0.867, 0.442, 0.404, 0.477, 0.324, 0.207, 0.625], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=0.471, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.48, 0.33, 0.319, 0.755, 0.554, 0.396, 0.509, 0.728], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.471, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.951, 0.433, 0.3, 0.349, 0.801, 0.237, 0.761, 0.079], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 7: Health=0.348, Actions=(0, 1, 1, 1, 1, 0, 0, 0), Obs=[0.281, 0.379, 0.2, 0.562, 0.694, 1.252, 0.596, 0.807], Rewards=[0.0, 0.0, 0.0, 1.017, 0.986, 0.0, 0.0, 0.0]\n",
      "\n",
      "--- Eval Episode 7 ---\n",
      "0.9572844792510224\n",
      "Step 0: Health=0.679, Actions=(0, 0, 1, 0, 0, 0, 0, 1), Obs=[0.971, 1.171, 1.078, 0.596, 0.733, 1.14, 1.227, 1.21], Rewards=[0.0, 0.0, 0.731, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 1: Health=0.437, Actions=(1, 1, 1, 1, 0, 0, 0, 0), Obs=[0.636, 0.522, 0.88, 0.134, 1.001, 0.307, 0.761, 0.791], Rewards=[0.682, 1.03, 0.0, 1.017, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 2: Health=0.437, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.642, 0.653, 0.562, 0.859, 0.224, 0.395, -0.014, 0.699], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=0.437, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.713, 0.65, -0.087, 0.469, 0.479, 0.418, 0.184, -0.05], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=0.437, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.879, 0.458, 0.633, 0.357, 0.052, 0.304, 0.107, 0.57], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=0.437, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.266, 0.199, 0.385, 0.433, 0.527, 0.603, 0.341, 0.3], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.283, Actions=(0, 0, 0, 0, 1, 1, 1, 0), Obs=[0.787, 0.656, 0.799, 0.553, 1.079, 0.35, 0.402, -0.06], Rewards=[0.0, 0.0, 0.0, 0.0, 0.986, 0.748, 0.965, 0.0]\n",
      "\n",
      "--- Eval Episode 8 ---\n",
      "0.8816158288970218\n",
      "Step 0: Health=0.600, Actions=(1, 0, 0, 1, 0, 1, 0, 0), Obs=[0.762, 0.181, 0.81, 0.672, 0.479, 1.168, 0.789, 0.856], Rewards=[0.682, 0.0, 0.0, 1.017, 0.0, 0.748, 0.0, 0.0]\n",
      "Step 1: Health=0.402, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.754, 0.905, 0.613, 0.318, 0.619, 0.73, 0.398, 0.452], Rewards=[0.0, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n",
      "Step 2: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.202, 0.226, 0.084, 0.235, -0.065, 0.418, 0.129, 0.238], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 3: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.552, 0.223, 0.374, -0.053, 0.53, 0.054, 0.576, 0.344], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 4: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.532, 0.235, 0.351, 0.506, 0.423, 0.628, 0.475, 0.893], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 5: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.651, 0.444, 0.781, 0.234, 0.392, 0.304, 0.085, 0.313], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 6: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.081, 0.726, 0.525, -0.193, 0.393, 0.467, 0.088, 0.265], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 7: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.233, 0.415, 0.489, 0.319, 0.571, 0.758, 0.369, 0.4], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 8: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.583, 0.776, 0.571, 0.135, 0.666, 0.903, -0.042, 0.297], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 9: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.677, -0.005, 0.124, 0.018, 0.119, 0.245, 0.456, 0.273], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 10: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.501, 0.228, 0.374, 0.555, 0.125, 0.489, 0.827, -0.13], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 11: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.475, 0.488, 0.553, -0.242, 0.555, 0.205, 0.13, 0.565], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 12: Health=0.402, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.052, -0.013, 0.305, 0.409, 0.273, 0.302, 0.337, 0.294], Rewards=[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Step 13: Health=0.350, Actions=(0, 0, 0, 1, 1, 0, 0, 0), Obs=[0.418, 0.571, 0.268, 0.965, 0.9, 0.336, 0.362, 0.247], Rewards=[0.0, 0.0, 0.0, 0.0, 0.986, 0.0, 0.0, 0.0]\n",
      "Step 14: Health=0.350, Actions=(0, 0, 0, 0, 0, 0, 0, 0), Obs=[0.549, 0.22, -0.125, -0.06, 0.139, 0.67, 0.267, 0.446], Rewards=[0.0, 0.0, 1.037, 0.0, 0.0, 0.0, 1.369, 0.0]\n",
      "\n",
      "--- Eval Episode 9 ---\n",
      "0.8223594453271651\n",
      "Step 0: Health=0.504, Actions=(0, 0, 0, 1, 1, 0, 1, 0), Obs=[0.637, 0.764, 0.948, 1.083, 0.583, 0.511, 0.78, 0.897], Rewards=[0.0, 0.0, 0.0, 1.017, 0.986, 0.0, 0.965, 0.0]\n",
      "Step 1: Health=0.292, Actions=(1, 1, 0, 0, 0, 0, 0, 1), Obs=[0.32, 0.686, 0.583, 0.199, 0.506, 0.888, 0.506, 0.906], Rewards=[0.682, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 1.498]\n"
     ]
    }
   ],
   "source": [
    "n_agents = 8\n",
    "obs_dim = 2\n",
    "env = BankRunEnv(n_agents=n_agents, T=15, alpha=1, health_min=0.3, sigma=0.25, R_func=None)\n",
    "\n",
    "q_net = CentralizedQNet(n_agents, obs_dim)\n",
    "target_q_net = CentralizedQNet(n_agents, obs_dim)\n",
    "target_q_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=2e-4)\n",
    "buffer = ReplayBuffer(50000)\n",
    "\n",
    "# Train\n",
    "train_centralized_dqn(env, n_agents, obs_dim, buffer, q_net, target_q_net,\n",
    "                        optimizer, episodes=5000, batch_size=128)\n",
    "\n",
    "# Evaluate\n",
    "evaluate(env, n_agents, q_net, episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
